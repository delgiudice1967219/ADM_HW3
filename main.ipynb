{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E4cWhSzcIK9"
   },
   "source": [
    "# ADM-HW3: GROUP #14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xavier Del Giudice, Alessio Iacono, Géraldine Valérie Maurer\n",
    "\n",
    "\n",
    "| STUDENT |   ID    |                 E-mail                  |\n",
    "| :-: |:-------:|:---------------------------------------:|\n",
    "| Xavier Del Giudice | 1967219 | delgiudice.1967219@studenti.uniroma1.it |\n",
    "| Alessio Iacono | 1870276 |   iacono.1870276@studenti.uniroma1.it   |\n",
    "| Géraldine Valérie Maurer | 1996887 |           gmaurer08@gmail.com            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:02:22.505080Z",
     "start_time": "2024-11-17T19:02:18.271918Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "illQRCTCZg4v",
    "outputId": "45f1c85d-efd3-40f6-eebf-364c9b9493de"
   },
   "outputs": [],
   "source": [
    "%pip install unidecode geopy plotly dash aiofiles aiohttp nltk ipywidgets requests bs4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:35:32.934563Z",
     "start_time": "2024-11-17T20:35:31.811958Z"
    }
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "import unicodedata\n",
    "from IPython.display import display, clear_output\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import ipywidgets as ipw\n",
    "from itertools import chain\n",
    "from functions import utils, engine, crawler, parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4za-YIOXRhy"
   },
   "source": [
    "# 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of Michelin restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script scrapes restaurant links from multiple pages of the Michelin guide website. \n",
    "\n",
    "In ```crawl_restaurant_links()``` function, it begins by sending a request to the first page and then parses the HTML content using BeautifulSoup. For each page, it searches for restaurant links, extracts them, and appends them to a list. The script then checks if there is a \"next page\" button and proceeds to the next page if available. The process continues until there are no more pages to scrape. \n",
    "\n",
    "Finally, all the extracted restaurant links are saved into a *.txt file* for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:03:11.923180Z",
     "start_time": "2024-11-17T19:02:29.543925Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWjzIJ4hUMUV",
    "outputId": "ddbcd81e-01c5-4aee-b499-4a398b4f1dea"
   },
   "outputs": [],
   "source": [
    "# Initial url of the Michelin guide where to start scraping\n",
    "base_url = 'https://guide.michelin.com/en/it/restaurants/page/'\n",
    "# Pathname of .txt file where URLs will be stored\n",
    "txt_out_pathname = 'michelin_restaurant_urls.txt'\n",
    "\n",
    "# Start txt crawling\n",
    "crawler.crawl_restaurant_links(base_url, txt_out_pathname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Crawl Michelin restaurant pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code **efficiently** downloads HTML pages using **asynchronous requests** and **concurrent processing** to maximize speed. \n",
    "\n",
    "It utilizes the aiohttp library to send multiple HTTP requests simultaneously, limiting the number of active connections to avoid overloading the server. The downloaded pages are saved into separate folders, with a new folder created every 20 pages, ensuring organized storage. \n",
    "\n",
    "**Asynchronous file operations** (***aiofiles***) are used to save the content, further enhancing performance. \n",
    "\n",
    "This approach significantly reduces the time needed to fetch multiple web pages compared to sequential downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:04:16.703063Z",
     "start_time": "2024-11-17T19:03:11.923180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pathname of .txt file where URLs are stored\n",
    "file_path = 'michelin_restaurant_urls.txt'\n",
    "# Pathname of directory where HTML pages will be stored\n",
    "output_dir = 'downloads'\n",
    "\n",
    "async def main():\n",
    "    # Load URLs from a file\n",
    "    urls = await crawler.load_urls(file_path)\n",
    "    # Download all URLs, organizing them into folders\n",
    "    await crawler.download_all(urls, output_dir)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code extracts detailed restaurant information from a collection of downloaded HTML files. \n",
    "\n",
    "In ```utils.iterate_folders(output_dir)``` function, we iterate over each page folder and, using BeautifulSoup, the script parses each HTML file and retrieves the restaurant's name, address, city, postal code, country, price range, cuisine type, description, facilities, accepted credit cards, phone number, and website. \n",
    "The data is organized into a dictionary for each restaurant and stored in a list. \n",
    "\n",
    "Once all the HTML files are processed, the data is converted into a Pandas DataFrame and saved as a tab-separated .tsv file. The script helps in organizing structured restaurant data from raw HTML pages for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:05:43.752085Z",
     "start_time": "2024-11-17T19:04:16.703063Z"
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing the downloaded HTML files\n",
    "output_dir = 'downloads'\n",
    "# Pathname of .tsv where final dataset will be stored\n",
    "tsv_pathname = 'restaurants_data.tsv'\n",
    "\n",
    "# Create a pandas DataFrame from data scraped from HTML pages and then save it to .tsv file\n",
    "df = pd.DataFrame(utils.iterate_folders(output_dir))\n",
    "df.to_csv(tsv_pathname, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Data saved to {tsv_pathname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert NaN values in the dataset to empty strings (as required)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:35:40.411941Z",
     "start_time": "2024-11-17T20:35:40.389694Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read dataset from .tsv and convert NaN values to empty string\n",
    "df = pd.read_csv('restaurants_data.tsv', sep='\\t')\n",
    "df = df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:45:03.089559Z",
     "start_time": "2024-11-17T19:45:03.077660Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collected 12 differents info for 1981 restaurants, and stored it in a dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:06:55.807364Z",
     "start_time": "2024-11-17T19:06:55.799769Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1i9zzfhXXhb"
   },
   "source": [
    "# 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utldng-rXa_9"
   },
   "source": [
    "## 2.0 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we create and use two search engines, implemented in the ```engine.py``` file that can be found in the folder ```functions```. The first search engine accepts a query and returns all the restaurants whose description contains all the terms in the query. The second search engine computes the cosine similarity between a given query and all the restaurant descriptions, and returns the top k restaurants that match the query best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to preprocess the restaurant descriptions. For this, we use the custom-made function ```preprocessing```, and save all pre-processed documents in a list of documents ```preprocessed_docs```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:55:42.046159Z",
     "start_time": "2024-11-17T20:55:40.336050Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_docs = defaultdict(list) # initialize defaultdict to store preprocessed docs\n",
    "for doc_id, doc in enumerate(df.description):\n",
    "  preprocessed_docs[doc_id] = engine.preprocessing(doc) # preprocess doc at position doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of what the preprocessed documents look like. They are now a list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:55:42.172743Z",
     "start_time": "2024-11-17T20:55:42.168582Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRWQ7HxDX1oY",
    "outputId": "1a78081f-abe8-4cf5-f25d-c9b19976a6f9"
   },
   "outputs": [],
   "source": [
    "# Test description\n",
    "text = '''After many years' experience in Michelin-starred restaurants, Luigi Tramontano and his wife Nicoletta\n",
    "have opened their first restaurant in the chef's native Gargnano. Previously a pasta factory, the building has been converted\n",
    "into an elegant, contemporary-style restaurant which has nonetheless retained its charming high ceilings.\n",
    "The cuisine is inspired by regional traditions which are reinterpreted to create gourmet dishes,\n",
    "all prepared with respect for the ingredients used and a strong focus on local produce.'''\n",
    "\n",
    "# Test preprocessing on test description\n",
    "print(engine.preprocessing(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DO0y6xQyfcyJ"
   },
   "source": [
    "## 2.1 Conjunctive Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faZun-BPfgJm"
   },
   "source": [
    "### 2.1.1 Create your Index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preprocessing, we take these two steps: \n",
    "1. save unique tokens in a DataFrame ```vocabulary_df``` that maps terms to unique integer IDs\n",
    "2. compute the inverted index for the documents. The inverted index is saved in a default dictionary, where the keys are the term IDs and the values are lists of document IDs of restaurant descriptions that contain the term. The code stores ```vocabulary_df``` in a csv file, and ```inverted_index``` in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:55:44.821089Z",
     "start_time": "2024-11-17T20:55:44.560416Z"
    },
    "id": "jCv8kfkcfnKl"
   },
   "outputs": [],
   "source": [
    "# 1. Vocabulary File\n",
    "\n",
    "# Retrieve the restaurants DataFrame\n",
    "df = pd.read_csv('restaurants_data.tsv', sep='\\t')\n",
    "\n",
    "doc_tokens = [] # initialize list to store all tokens\n",
    "\n",
    "# Find unique tokens\n",
    "for doc in preprocessed_docs.values():\n",
    "  doc_tokens.extend(doc)\n",
    "  doc_tokens = list(set(doc_tokens)) # remove duplicates\n",
    "\n",
    "vocabulary_dict = {term: i for i,term in enumerate(doc_tokens)} # dictionary of all vocabulary terms\n",
    "vocabulary_df = pd.DataFrame({'term': vocabulary_dict.keys(), 'term_id': vocabulary_dict.values()}) # dataframe that maps terms to IDs\n",
    "\n",
    "vocabulary_df.to_csv('vocabulary.csv', index=False) # save vocabulary dataframe in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:55:46.041445Z",
     "start_time": "2024-11-17T20:55:45.978566Z"
    },
    "id": "QI3YYNZMhpC_"
   },
   "outputs": [],
   "source": [
    "# 2. Inverted Index\n",
    "\n",
    "inverted_index = defaultdict(list) # initialize inverted_index dictionary\n",
    "\n",
    "# Compute the inverted_index\n",
    "for doc_id, row in enumerate(df.description):\n",
    "  tokens = set(preprocessed_docs[doc_id]) # preprocessed description\n",
    "  for token in tokens: # eliminate duplicates\n",
    "    # Look up the term_id of the current term/token\n",
    "    term_id = vocabulary_dict[token]\n",
    "    # If the doc_id is not in the term_id's list in inverted_index, add it\n",
    "    if doc_id not in inverted_index[term_id]:\n",
    "      inverted_index[term_id].append(doc_id)\n",
    "\n",
    "# Save the inverted_index dictionary to a file\n",
    "with open(\"inverted_index.pkl\", \"wb\") as file:\n",
    "    pickle.dump(inverted_index, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we allow the user to input a query. After clicking on search, the first search engine will be triggered to retrieve all restaurants that contain in their description the same terms as the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:09:17.399935Z",
     "start_time": "2024-11-17T19:09:17.380576Z"
    },
    "id": "mstIv_0qlqXw"
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "# re-load inverted index in case it was modified somewhere\n",
    "with open('inverted_index.pkl', 'rb') as file:\n",
    "    inverted_index = pickle.load(file)\n",
    "\n",
    "# Text input field for query\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your query',\n",
    "    description='Query:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Search button\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to handle button press\n",
    "def on_search_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()  # clear previous output if there are any\n",
    "        query = text_input.value\n",
    "        if query.strip():  # Check if there's an input\n",
    "            display(engine.find_restaurants(query, vocabulary_df, inverted_index, df)) # display query results\n",
    "        else:\n",
    "            print(\"Please enter something to search for.\")\n",
    "\n",
    "# Link the function to the button\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "display(text_input, search_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the interactive search engine cannot be rendered, here is an example output for the query 'modern seasonal cuisine':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the interactive search engine cannot be rendered, here is an example output for the query 'modern seasonal cuisine':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:17:18.775893Z",
     "start_time": "2024-11-17T19:17:18.762318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = 'modern seasonal cuisine'\n",
    "\n",
    "# Find restaurants with first search engine\n",
    "display(engine.find_restaurants(query, vocabulary_df, inverted_index, df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92JOh6oFyPuz"
   },
   "source": [
    "## 2.2 Ranked Search Engine with TF-IDF and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXxrWEbzyVV2"
   },
   "source": [
    "### 2.2.1 Inverted Index with TF-IDF Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we implement the second search engine, and for that we first compute the inverted index with TF-IDF scores using the custom-made function ```tf_idf``` and save the ```updated_inverted_index``` in a pickle file. The function ```tf_idf``` computes the TF-IDF scores of a specific term for each preprocessed document, returning a list of tuples ```(doc_id, tf_idf)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:19:15.314157Z",
     "start_time": "2024-11-17T19:19:15.125028Z"
    },
    "collapsed": true,
    "id": "KNv23rrBWtxx"
   },
   "outputs": [],
   "source": [
    "# Preliminary steps\n",
    "n = len(preprocessed_docs)\n",
    "updated_inverted_index = defaultdict(list) # initialize default dictionary to store the inverted_index values with TF-IDF scores\n",
    "inverted_index_copy = inverted_index.copy() # Create a copy of the inverted_index to iterate over\n",
    "\n",
    "# Compute updated_inverted_index\n",
    "for term_id, docs in inverted_index_copy.items():\n",
    "  tf_idf_scores = engine.tf_idf(int(term_id), inverted_index, preprocessed_docs, vocabulary_df, n)\n",
    "  updated_inverted_index[term_id] = list(zip(docs, tf_idf_scores))\n",
    "\n",
    "with open('updated_inverted_index.pkl', 'wb') as file:\n",
    "    pickle.dump(updated_inverted_index, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we retrieve from ```updated_inverted_index``` the TF-IDF scores related to documents, and memorize only the tuples ```(term, tf-idf)``` where tf-idf $\\neq 0$ for each document in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:19:16.998039Z",
     "start_time": "2024-11-17T19:19:16.874979Z"
    },
    "collapsed": true,
    "id": "rLbjRaqLS9bj"
   },
   "outputs": [],
   "source": [
    "# Compute the TF-IDF vectors of all documents and store them in a pickle file\n",
    "doc_tf_idf_scores = defaultdict(list) # initialize dictionary to store non-zero TF-IDF scores for each document\n",
    "\n",
    "for term_id, docs_scores in updated_inverted_index.items():\n",
    "  for doc_id, tf_idf_score in docs_scores:\n",
    "    if tf_idf_score != 0:\n",
    "      doc_tf_idf_scores[doc_id].append((term_id,tf_idf_score))\n",
    "  doc_tf_idf_scores[doc_id].sort(key=lambda x: x[0]) # sort the terms\n",
    "\n",
    "with open('doc_tf_idf_scores.pkl', 'wb') as file:\n",
    "    pickle.dump(doc_tf_idf_scores, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we enable the user to input a text query, and return the top-k ranked restaurants by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:19:19.352439Z",
     "start_time": "2024-11-17T19:19:19.333365Z"
    },
    "id": "LXzqp1nkQ661"
   },
   "outputs": [],
   "source": [
    "# re-load inverted index in case it was modified somewhere\n",
    "with open('inverted_index.pkl', 'rb') as file:\n",
    "    inverted_index = pickle.load(file)\n",
    "\n",
    "# Text input field for query\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your query',\n",
    "    description='Query:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Search button\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to handle button press\n",
    "def on_search_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()  # clear previous output if there are any\n",
    "        query = text_input.value\n",
    "        if query.strip():  # Check if there's an input\n",
    "            k = 10\n",
    "            display(engine.top_k_restaurants(query, inverted_index, vocabulary_dict, doc_tf_idf_scores, df, n, k)) # display query results\n",
    "        else:\n",
    "            print(\"Please enter something to search for.\")\n",
    "\n",
    "# Link the function to the button\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "display(text_input, search_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the interactive search engine cannot be rendered, here is an example output prompted by the query 'modern seasonal cuisine':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:19:47.564976Z",
     "start_time": "2024-11-17T19:19:47.525928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = 'modern seasonal cuisine'\n",
    "\n",
    "# Find restaurants with first search engine\n",
    "display(engine.top_k_restaurants(query, inverted_index, vocabulary_dict, doc_tf_idf_scores, df, n, k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define a New Score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A user interface(UI) is used to consider the user's additive information, which allows the user to enter the fields they prefer to find the restaurant.\n",
    "We will use this additional information to predicate some of the rows in our dataset.\n",
    "Since all restaurants are filtered by text initially, we want to give lower weight to the similarity between the description and the input query.  \n",
    "Rather we want to give higher weight to user input details, which are the key to finding restaurants to which a user can bring interest.\n",
    "For example, if I were to search for a restaurant and I have a particular desire to eat that cuisine I would want those as the first search results.\n",
    "So we increase the score for the rows of the dataset for which we have a match for cuisine type.\n",
    "We also increase multiplying a weight for the number of facilities and services matched with the input divided by the number of facilities the user inserted. And we increase the score if the price range is matched as well.\n",
    "We can also find this behavior in very popular search engines, for example amazon search, prefers user-selected filters over the user's even detailed content in the search bar.\n",
    "\n",
    "The results in the end turn out to be better, more accurate. We also have to consider that in the previous Engine based on the cosine similarity we are just considering the score based on the similarity between the input query and the description which is way larger than a possible query. Instead now we are building our own custom score where we give more importance to the details, in fact we have bigger score for the ones that perfectly fit the search. We are basically considering more fields so its obvious that the results are more accurated. But there is also to note that at the computational level this type of search is less performant since we have to initially load all the restaurants that respect the query input and then create a new score based on matching the user input and finally sort to select only the first k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this search, it was necessary to update the function ```find_restaurants``` in ```find_restaurants_updated```.\n",
    "To allow all the columns to be used as well, considering also the usage of the following function in the map visualization.\n",
    "\n",
    "Finally, the ```find_top_custom_restaurants``` function was created, available in the ```engine.py``` file. This return the DataFrame ordered by the new customScore just created.\n",
    "\n",
    "Through the interface, users can interact with the application to obtain customized results based on their selections. Each time the \"Search\" button is clicked, a Pandas dataframe is returned with the requested data.\n",
    "\n",
    "All inputs can be cleared using the \"Clear\" button. Users can specify several parameters, including:\n",
    "\n",
    "- Text to input for the search\n",
    "- Cuisine type\n",
    "- Desired facilities and services\n",
    "- Price range\n",
    "- Number of results to display  \n",
    "\n",
    "For each search, a ```top_k_result.tsv``` file is generated, allowing the main results to be saved and used for map visualization. These features are designed to work seamlessly together, ensuring a smooth and efficient user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:55:58.098233Z",
     "start_time": "2024-11-17T20:55:58.095869Z"
    }
   },
   "outputs": [],
   "source": [
    "from functions.search_restaurant_ui import SearchRestaurantUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:56:10.874672Z",
     "start_time": "2024-11-17T20:56:10.846194Z"
    }
   },
   "outputs": [],
   "source": [
    "search_ui = SearchRestaurantUI(df, vocabulary_df, inverted_index)\n",
    "search_ui.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the previous points, we can not visualize the UI just created.\n",
    "But if you would like to use it, you can download the code and run it in local.\n",
    "In the next cell you will see the output generated by the parameters:  \n",
    "\n",
    "**Description searched** : modern seasonal cuisine  \n",
    "\n",
    "**Cuisine Type** : Italian Contemporary, Creative  \n",
    "\n",
    "**Facilities & Services** : Air Conditioning, Car Park, Interesting Wine List  \n",
    "\n",
    "**Price Range** : €€€\n",
    "\n",
    "**Number of Results** : 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:59:05.165165Z",
     "start_time": "2024-11-17T20:59:04.634306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = {\n",
    "    'description' : 'modern seasonal cuisine',\n",
    "    'cuisineType' : 'Italian Contemporary, Creative',\n",
    "    'facilitiesServices' : ['Air conditioning', 'Car park', 'Interesting wine list'],\n",
    "    'priceRange' : '€€€',\n",
    "    'num_results' : 10\n",
    "}\n",
    "\n",
    "# Find restaurants with first search engine\n",
    "display(engine.find_top_custom_restaurants(query, vocabulary_df, inverted_index, df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualizing the Most Relevant Restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Geocode Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script extracts restaurant information from the directory *downloads*, where each restaurant HTML page is stored, processes it, and saves the data in a tabular format (TSV). \n",
    "\n",
    "Here's a breakdown of its functionality:\n",
    "\n",
    "- **Restaurant Data Extraction**: ```utils.iterate_geo_folders(output_dir) ```\n",
    "\n",
    "    Uses **BeautifulSoup** to parse each .html file, then locates and processes a JSON script tag within the HTML to extract specific restaurant information, including:\n",
    "    - Name (*restaurantName*)\n",
    "    - Region (*addressRegion*)\n",
    "    - Latitude (*latitude*)\n",
    "    - Longitude (*longitude*) \n",
    "    \n",
    "- **Region Translation** ``` utils.translateRegions() ``` :\n",
    "    Since some regions have their names in English, we map and replace them with their Italian equivalents using a predefined dictionary.\n",
    "\n",
    "- **Data Storage**:\n",
    "    Compiles the extracted data into a list of dictionaries and convert this list into a Pandas DataFrame.\n",
    "- **Data Export**:\n",
    "    Saves the processed restaurant data to a file named geodata.tsv in tabular format (TSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:04:57.355854Z",
     "start_time": "2024-11-17T20:59:09.753132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Directory where .html files are stored\n",
    "output_dir = 'downloads'\n",
    "# Filename of tsv where geodata (region, latitude and longitude) of each restaurant will be saved\n",
    "tsv_filename = 'geodata.tsv'\n",
    "\n",
    "# Create a pandas DataFrame from scraped geodata\n",
    "df = pd.DataFrame(utils.iterate_geo_folders(output_dir))\n",
    "\n",
    "# Translation of italian region from english to italian name (needed for map)\n",
    "df['region'] = utils.translateRegions(df['region'])\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv(tsv_filename, sep='\\t', index=False)\n",
    "print(f\"Data saved to {tsv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge the information about the restaurants and their geo-localition info we need to merge the two different table: ```restaurants_data.tsv``` and ```geodata.tsv```. </br>\n",
    "To do this we use pandas command: ```merge```, but first, to avoid duplicates and bias we defined an attribute of join. </br>\n",
    "It need to be unique for each row of the two dataset. So we defined *restaurantId* that is an autoincrement value for each row of the dataset (primarey key).\n",
    "We also sort all rows of the two datasets by *restaurant name* to have a perfect match within the restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:04:59.728741Z",
     "start_time": "2024-11-17T21:04:59.707067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve restaurants_data.tsv dataframe\n",
    "df = pd.read_csv('restaurants_data.tsv', sep='\\t')\n",
    "\n",
    "# Sort DF by 'restaurantName' column\n",
    "df = df.sort_values(by=\"restaurantName\")\n",
    "\n",
    "# Assign numerical and sequential restaurantId to each restaurant \n",
    "df['restaurantId'] = range(1, len(df) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the creation of the *restaurantId* column for the ```geodata.tsv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:05:01.325296Z",
     "start_time": "2024-11-17T21:05:01.313977Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve geodata.tsv dataframe\n",
    "df_geo = pd.read_csv(\"geodata.tsv\", sep='\\t')\n",
    "\n",
    "# Sort DF by 'restaurantName' column\n",
    "df_geo = df_geo.sort_values(by=\"restaurantName\")\n",
    "\n",
    "# Assign numerical and sequential restaurantId to each restaurant \n",
    "df_geo['restaurantId'] = range(1, len(df) + 1)\n",
    "\n",
    "print(df_geo.shape)\n",
    "df_geo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge this two table, on the specified attribute (*restaurantId*), obtaining a new pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:05:05.249755Z",
     "start_time": "2024-11-17T21:05:05.241994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge dataframes, matching rows by 'restaurantID' (present in both DF).\n",
    "df_final = pd.merge(df, df_geo, on='restaurantId', how='right')\n",
    "\n",
    "print(df_final.shape)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output above the column restaurantName seems to be not unique, that's way we didn't use this column as attribute of join.\n",
    "This is probably caused by the data gathering process, in which is possibile that some character differs from the two tables.\n",
    "So, we decided to check how many rows differs in the relative column, then we drop one of the two and we keep it as \"restaurantName\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:05:07.189379Z",
     "start_time": "2024-11-17T21:05:07.181591Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_final.rename(columns={'restaurantName_x': 'restaurantName'})\n",
    "\n",
    "# Drop column 'restaurantName_y'\n",
    "df_final = df_final.drop(columns=['restaurantName_y'])\n",
    "\n",
    "print(df_final.shape)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how is our ```df_final``` composed. We have all the information about the restaurant and we also added the information about its region and latidute and longitude.\n",
    "This allow us to represent the restaurant and all its information as point in our map plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Map Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Dash web application** designed to display and interact with restaurant data on maps of Italy and its regions. <br>\n",
    "\n",
    "Here's a summary:\n",
    "\n",
    "- ***Purpose***: <br>\n",
    "    The app shows a choropleth map of Italian regions (left) and allows users to click on a region to view detailed restaurant information on a zoomed-in map (right). <br>\n",
    "    Users can optionally filter restaurants to show only \"Top-K\" entries from a pre-defined dataset in ***Point 3*** (*top_k_result.tsv*).\n",
    "\n",
    "- ***Key Features***: <br>\n",
    "    The left map (*Italy map*) displays all regions and allows selecting a region via clicks.<br>\n",
    "    The right map (*Region map*) shows restaurants located in the selected region, categorized by price range and marked with color-coded pins.<br>\n",
    "    Hovering over restaurant markers reveals details such as name, city, address, and cuisine type.\n",
    "\n",
    "- ***Data Handling***: <br>\n",
    "    The app uses GeoPandas for loading a GeoJSON file of Italian regions and ensures it has the proper CRS (EPSG:4326) for accurate centroid calculations, so that Region map is always displayed centered<br>\n",
    "    Restaurant data is filtered based on user input (e.g., Top-K filter or region selection).\n",
    "\n",
    "- ***Interactive Components***: <br>\n",
    "    A checklist allows toggling the \"Top-K\" filter. <br>\n",
    "    Maps are updated dynamically using Dash callbacks based on user interactions like region clicks or checkbox toggles.\n",
    "\n",
    "    **N.B.** *If we change parameters for which scores are calculated, so if our \"Top-k\" restaurants changes, just toggle two times to the \"Show Top-K\" checkbox to update the map*\n",
    "\n",
    "    <br>\n",
    "\n",
    "- ***Visualizations***: <br>\n",
    "    Built using Plotly’s choropleth_mapbox for choropleth maps and scattermapbox for restaurant markers.\n",
    "\n",
    "- ***Technical Notes***: <br>\n",
    "    The application operates as a standalone Dash server and can be used either in a Jupyter Notebook or rendered in an external web browser.\n",
    "    \n",
    "    To control where the map is displayed, modify the last line of code in the cell accordingly:\n",
    "    \n",
    "    - **External Browser (DEFAULT)**\n",
    "        ``` python\n",
    "        app.run_server(jupyter_mode='external', debug=True)\n",
    "        ```\n",
    "    - **Jupyter Notebook**\n",
    "        ``` python\n",
    "        app.run_server(debug=True)\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:05:16.595341Z",
     "start_time": "2024-11-17T21:05:14.353057Z"
    }
   },
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load GeoJSON file of Italian regions; necessary to draw region borders on the Italy map\n",
    "gdf = gpd.read_file(\"./GeoJson/limits_IT_regions.geojson\")\n",
    "\n",
    "# Update the \"Coordinate Reference System\" (CRS) of the GeoDataFrame\n",
    "# This ensures that the centroids of the regions are calculated correctly\n",
    "if gdf.crs != \"EPSG:4326\":\n",
    "    gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Extract the list of region names from the GeoDataFrame\n",
    "region_names = gdf['reg_name'].tolist()\n",
    "\n",
    "# Create the Dash app instance (Dash provides an API for creating web apps in Python)\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define the app layout, which includes filters and maps\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "        # Add a checklist for filtering Top-K restaurants\n",
    "        dcc.Checklist(\n",
    "            id='show-top-k',\n",
    "            options=[{'label': 'Show Top-K Restaurants Only', 'value': 'top_k'}],\n",
    "            value=[],\n",
    "            inline=True\n",
    "        ),\n",
    "    ]),\n",
    "    \n",
    "    # Create two maps: \n",
    "    # 1. An Italy map (left) where the user can click on a region.\n",
    "    # 2. A detailed region map (right) showing restaurants in the selected region.\n",
    "    html.Div([\n",
    "        dcc.Graph(id='italy-map', clickData=None, style={'width': '50vw', 'height': '100vh'}),\n",
    "        dcc.Graph(id='region-map', style={'width': '50vw', 'height': '100vh'})\n",
    "    ], style={'display': 'flex', 'flex-direction': 'row'})\n",
    "])\n",
    "\n",
    "# Initialize a global variable to store filtered restaurant data\n",
    "filtered_restaurants = df_final\n",
    "\n",
    "# Define a callback to update both maps based on user interaction\n",
    "@app.callback(\n",
    "    [Output('italy-map', 'figure'),\n",
    "     Output('region-map', 'figure')],\n",
    "    [Input('italy-map', 'clickData'),\n",
    "     Input('show-top-k', 'value')]\n",
    ")\n",
    "# Function to update maps:\n",
    "# - The left map (Italy) allows selecting a region.\n",
    "# - Clicking on a region updates the right map with detailed restaurant information.\n",
    "def update_maps(clickData, show_top_k):\n",
    "    global filtered_restaurants\n",
    "    \n",
    "    # Filter restaurants if the \"Top-K\" checkbox is checked\n",
    "    if 'top_k' in show_top_k:\n",
    "        top_k_df = pd.read_csv('top_k_result.tsv', sep='\\t')\n",
    "        filtered_restaurants = df_final[df_final['restaurantName'].isin(top_k_df['restaurantName'])]\n",
    "    else:\n",
    "        filtered_restaurants = df_final\n",
    "\n",
    "    # Variable to track the currently selected region\n",
    "    selected_region = None\n",
    "    if clickData:\n",
    "        selected_region = clickData['points'][0]['location']\n",
    "\n",
    "    # Create the Italy map\n",
    "    italy_map = px.choropleth_mapbox(\n",
    "        geojson=json.loads(gdf.to_json()),  # Use the GeoJSON for regions\n",
    "        locations=gdf['reg_name'],         # Match regions using the \"reg_name\" column\n",
    "        featureidkey=\"properties.reg_name\",\n",
    "        color_continuous_scale=['grey'],   # Light grey color for regions\n",
    "        opacity=0.2,                       # Semi-transparent regions\n",
    "        mapbox_style=\"carto-positron\",     # Map styling\n",
    "        zoom=4.5,                          # Default zoom level for Italy\n",
    "        center={\"lat\": 41.8719, \"lon\": 12.5674},  # Center of Italy\n",
    "        title=\"Map of the Italian Regions\" # Map title\n",
    "    )\n",
    "    \n",
    "    # Add restaurant markers to the Italy map if \"Top-K\" is enabled and a region is selected\n",
    "    if 'top_k' in show_top_k and selected_region:\n",
    "        for price_range, color in [('€', 'teal'), ('€€', 'red'), ('€€€', 'mediumpurple'), ('€€€€', 'peru')]:\n",
    "            # Filter restaurants by price range\n",
    "            price_restaurants = filtered_restaurants[filtered_restaurants['priceRange'] == price_range].copy()\n",
    "\n",
    "            # Generate hover info for each restaurant\n",
    "            priceBox = price_restaurants.apply(\n",
    "                lambda row: (\n",
    "                    f\"<b>Name:</b> {row['restaurantName']}<br>\"\n",
    "                    f\"<b>City:</b> {row['city']}<br>\"\n",
    "                    f\"<b>Address:</b> {row['address']}<br>\"\n",
    "                    f\"<b>Types of cuisine: </b> {row['cuisineType']}<br>\"\n",
    "                ), axis=1)\n",
    "            price_restaurants['hover_info'] = priceBox if not priceBox.empty else None\n",
    "\n",
    "            # Add restaurant markers to the Italy map\n",
    "            italy_map.add_scattermapbox(\n",
    "                lat=price_restaurants['latitude'].tolist(),\n",
    "                lon=price_restaurants['longitude'].tolist(),\n",
    "                mode='markers',\n",
    "                marker=dict(size=10, color=color, opacity=0.8),\n",
    "                text=price_restaurants['restaurantName'],\n",
    "                name=price_range,\n",
    "                hoverlabel=dict(\n",
    "                    bgcolor=\"whitesmoke\", \n",
    "                    bordercolor=color, \n",
    "                    font=dict(\n",
    "                        color=color\n",
    "                    )\n",
    "                ),\n",
    "                hovertemplate=\"%{customdata[0]}\",\n",
    "                customdata=price_restaurants[['hover_info']].values\n",
    "            )\n",
    "    \n",
    "    # Highlight the selected region with a red border (NOT WORKING)\n",
    "    # if selected_region:\n",
    "    #     italy_map.update_traces(marker_line_color=\"red\", selector=dict(location=selected_region))\n",
    "    \n",
    "    # Generate a map for the selected region\n",
    "    if selected_region:\n",
    "        region_gdf = gdf[gdf['reg_name'] == selected_region]\n",
    "        \n",
    "        # Calculate the centroid of the region for map centering\n",
    "        region_gdf_proj = region_gdf.to_crs(epsg=32632)\n",
    "        centroid_proj = region_gdf_proj.geometry.centroid.iloc[0]\n",
    "        centroid = gpd.GeoSeries([centroid_proj], crs=\"EPSG:32632\").to_crs(\"EPSG:4326\").iloc[0]\n",
    "        center_lat, center_lon = centroid.y, centroid.x\n",
    "\n",
    "        # Filter restaurants in the selected region\n",
    "        filtered_restaurants_region = filtered_restaurants[filtered_restaurants['region'] == selected_region]\n",
    "\n",
    "        # Create the region map\n",
    "        region_map = px.choropleth_mapbox(\n",
    "            geojson=json.loads(region_gdf.to_json()),  # GeoJSON for the selected region\n",
    "            locations=[selected_region],\n",
    "            featureidkey=\"properties.reg_name\",\n",
    "            mapbox_style=\"carto-positron\",\n",
    "            color_discrete_sequence=['red'],  # Red for the selected region\n",
    "            opacity=0.1,\n",
    "            zoom=6,\n",
    "            center={\"lat\": center_lat, \"lon\": center_lon},\n",
    "            title=f\"Map of {selected_region} with Restaurants\"  # Region map title\n",
    "        )\n",
    "        \n",
    "        # Add restaurant markers for the selected region\n",
    "        for price_range, color in [('€', 'teal'), ('€€', 'red'), ('€€€', 'mediumpurple'), ('€€€€', 'peru')]:\n",
    "            price_restaurants = filtered_restaurants_region[filtered_restaurants_region['priceRange'] == price_range].copy()\n",
    "\n",
    "            priceBox = price_restaurants.apply(\n",
    "                lambda row: (\n",
    "                    f\"<b>Name:</b> {row['restaurantName']}<br>\"\n",
    "                    f\"<b>City:</b> {row['city']}<br>\"\n",
    "                    f\"<b>Address:</b> {row['address']}<br>\"\n",
    "                    f\"<b>Types of cuisine: </b> {row['cuisineType']}<br>\"\n",
    "                ), axis=1)\n",
    "            price_restaurants['hover_info'] = priceBox if not priceBox.empty else None\n",
    "\n",
    "            region_map.add_scattermapbox(\n",
    "                lat=price_restaurants['latitude'].tolist(),\n",
    "                lon=price_restaurants['longitude'].tolist(),\n",
    "                mode='markers',\n",
    "                marker=dict(size=10, color=color, opacity=0.8),\n",
    "                text=price_restaurants['restaurantName'],\n",
    "                name=price_range,\n",
    "                hoverlabel=dict(\n",
    "                    bgcolor=\"whitesmoke\", \n",
    "                    bordercolor=color, \n",
    "                    font=dict(\n",
    "                        color=color\n",
    "                    )\n",
    "                ),\n",
    "                hovertemplate=\"%{customdata[0]}\",\n",
    "                customdata=price_restaurants[['hover_info']].values\n",
    "            )\n",
    "    else:\n",
    "        # If no region is selected, return an empty map\n",
    "        region_map = {}\n",
    "\n",
    "    return italy_map, region_map\n",
    "\n",
    "# Run the Dash app (app could be used also in this notebook, just remove \"jupyter_mode='external'\")\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(jupyter_mode='external', debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. BONUS: Advanced Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an advanced restaurant search, we created a user interface to respond to user requests.\n",
    "First of all created an index for each of the insertable text fields:\n",
    "- **restaurantName** \n",
    "- **city**\n",
    "- **cuisineType**\n",
    "  \n",
    "For each of these, the relative cosine similarity score with respect to the respective columns was calculated. However, we used a system of weights based on the text fields entered by the user, because we want to give more importance to some of the fields based on those entered. All possible situations with which a user may interact with the interface were considered, and a weight was assigned to each of these.\n",
    "To finally aggregate the cosine similarity scores we considered the sum over those calculated for each field and then divided by the number of fields entered.\n",
    "We prefer the restaurantName, because if a user knows the restaurant where he wants to eat he should have this result as the first one and then based on the other filters entered he should still have other results to have more choice.\n",
    "Instead, we respect the other filters:\n",
    "- **Price Range Filter**\n",
    "- **Region Filter**\n",
    "- **Accepted Credit Cards**\n",
    "- **Services and Facilities**  \n",
    "\n",
    "We considered the intersection of the results obtained for the similarity score with respect to these filters. Thus, the restaurants returned are those that meet all filtering conditions.\n",
    "\n",
    "To obtain the described results, we used the function ```enanched_search_restaurants``` which uses the function ```get_weighted_similarity``` (where you can also find the weight schema) to calculate the weighted cosine similarity for each restaurant, both in the ```engine.py``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:22:39.291360Z",
     "start_time": "2024-11-17T21:22:39.226964Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect all the possible values for Credit Cards, Services and Facilities, Regions, Price Range\n",
    "unique_credit_cards = sorted(list(set(chain(*df_final['creditCards'].apply(eval)))))\n",
    "unique_services_facilities = sorted(list(set(chain(*df_final['facilitiesServices'].apply(eval)))))\n",
    "regions = sorted(pd.Series(df_final[\"region\"]).dropna().drop_duplicates().tolist())\n",
    "price_ranges = sorted(df_final['priceRange'].unique().tolist())\n",
    "\n",
    "# Widget input\n",
    "restaurant_name = ipw.Text(placeholder=\"Type restaurant name\", description=\"Restaurant:\")\n",
    "city = ipw.Text(placeholder=\"Type city\", description=\"City:\")\n",
    "cuisine_type = ipw.Text(placeholder=\"Type cuisine\", description=\"Cuisine:\")\n",
    "price_range = ipw.SelectionRangeSlider(\n",
    "    options=price_ranges,\n",
    "    value=[price_ranges[0], price_ranges[-1]],\n",
    "    description=\"Price Range:\"\n",
    ")\n",
    "\n",
    "# Checkbox for regions, CreditCards, Facilities & Services\n",
    "region_checkboxes = [ipw.Checkbox(value=False, description=option) for option in regions]\n",
    "region_box = ipw.VBox(region_checkboxes)\n",
    "\n",
    "credit_card_checkboxes = [ipw.Checkbox(value=False, description=option) for option in unique_credit_cards]\n",
    "credit_card_box = ipw.VBox(credit_card_checkboxes)\n",
    "\n",
    "facility_checkboxes = [ipw.Checkbox(value=False, description=option) for option in unique_services_facilities]\n",
    "facility_box = ipw.VBox(facility_checkboxes)\n",
    "\n",
    "# Search button\n",
    "search_button = ipw.Button(description=\"Search\", button_style=\"success\")\n",
    "output = ipw.Output()\n",
    "\n",
    "def on_search_button_click(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        search_params = {\n",
    "            \"restaurant_name\": restaurant_name.value,\n",
    "            \"city\": city.value,\n",
    "            \"cuisine_type\": cuisine_type.value,\n",
    "            \"price_range\": price_range.value,\n",
    "            \"regions\": [rc.description for rc in region_checkboxes if rc.value],\n",
    "            \"credit_cards\": [ccc.description for ccc in credit_card_checkboxes if ccc.value],\n",
    "            \"facilities\": [fc.description for fc in facility_checkboxes if fc.value]\n",
    "        }\n",
    "        results = engine.enanched_search_restaurants(df_final, search_params)\n",
    "        if results is not None:\n",
    "            display(results)\n",
    "        else:\n",
    "            print(\"No results found.\")\n",
    "    \n",
    "search_button.on_click(on_search_button_click)\n",
    "\n",
    "# Widget display (Show UI)\n",
    "display(restaurant_name, city, cuisine_type, price_range)\n",
    "display(ipw.Label(\"Regions:\"))\n",
    "display(region_box)\n",
    "display(ipw.Label(\"Credit Cards:\"))\n",
    "display(credit_card_box)\n",
    "display(ipw.Label(\"Services and Facilities:\"))\n",
    "display(facility_box)\n",
    "display(search_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:41:22.982356Z",
     "start_time": "2024-11-17T20:41:21.385566Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example query\n",
    "search_params = {\n",
    "    'restaurant_name' : 'Al',\n",
    "    'city' : 'Venice',\n",
    "    'cuisine_type' : 'Seafood, Venetian',\n",
    "    'price_range' : ['€€', '€€€€'],\n",
    "    'regions' : ['Veneto'],\n",
    "    'credit_cards' : ['mastercard', 'visa'],\n",
    "    'facilities' : ['Air conditioning', 'Terrace']\n",
    "}\n",
    "\n",
    "# Find restaurants with first search engine\n",
    "display(engine.enanched_search_restaurants(df_final, search_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Question (AQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Pseudocode\n",
    "\n",
    "The intuition is that, starting from the origin (0, 0) in the first quadrant where all packages are located, the first package is always reachable. Given that we can only move up or right, we can only reach packages that are above or to the right of the current one. Therefore, for each package reached, we need to check the next one: if it is below or to the left, we can print \"NO\" because it’s unreachable. Otherwise, we proceed, but to ensure the lexicographically smallest path, we need to sort all packages in ascending order by their coordinates.\n",
    "\n",
    "Below is the pseudocode of an algorithm that solves this problem:\n",
    "\n",
    "$ \\mathbf{PackageCollector(t)} $\n",
    "\n",
    "(0) $ \\textbf{for text case t}: $\n",
    "\n",
    "* (1) $ read(n) $\n",
    "\n",
    "* (2) $ \\text{packages} ← \\text{[ ]} $\n",
    "\n",
    "* (3) $ \\textbf{for i=1 to n:} $ // $O(n)$\n",
    "\n",
    " * (4) $ read(x[i]), read(y[i]) $\n",
    " * (5) $ \\text{packages} \\leftarrow \\text{packages} + [(x[i], y[i])] $\n",
    "* (6) $ \\textbf{Sort} \\text{ packages by x and then y coordinate in ascending order}$ // $ O(n \\log(n)) $\n",
    "* (7) $ \\text{x_current} \\leftarrow 0 $\n",
    "* (8) $ \\text{y_current} \\leftarrow 0 $\n",
    "* (9) $ \\text{path } \\leftarrow \\text{' '}$\n",
    "* (10) $ \\text{possible} \\leftarrow \\text{TRUE} $\n",
    "\n",
    "* (11) $\\textbf{for i=1 to n:} $ // $O(n)$\n",
    "\n",
    "  * (12) $ \\textbf{if } \\mathbf{x[i]<}\\textbf{x_current or } \\mathbf{y[i]<}\\textbf{y_current:} $\n",
    "    * (13) $ \\text{possible} \\leftarrow \\text{FALSE} $\n",
    "    * (14) $ break $\n",
    "  * $ \\textbf{end if} $\n",
    "  * (15) $ \\text{num_right_moves} \\leftarrow x[i]-\\text{x_current} $\n",
    "  * (16) $ \\text{num_up_moves} \\leftarrow y[i]-\\text{y_current} $\n",
    "  * (17) $ \\text{path} \\leftarrow \\text{path} + \\text{'R'} \\cdot \\text{num_right_moves} $\n",
    "  * (18) $ \\text{path} \\leftarrow \\text{path} + \\text{'U'} \\cdot \\text{num_up_moves} $\n",
    "  * (19) $ \\text{x_current} \\leftarrow x[i]$\n",
    "  * (20) $ \\text{y_current} \\leftarrow y[i]$\n",
    "* $ \\textbf{end for} $\n",
    "* (21) $ \\textbf{if possible:} $\n",
    "    * (22) $ output(\\text{'YES'}) $\n",
    "    * (23) $ output(\\text{path}) $\n",
    "* $ \\textbf{end if} $\n",
    "* (24) $ \\textbf{else:} $\n",
    "  * (25) $ output(\\text{'NO'}) $\n",
    "\n",
    "$ \\textbf{end for} $\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Proof of Correctness\n",
    "\n",
    "The algorithm aims to determine a path from the origin $(0,0)$ to each package coordinate $(x[i],y[i])$, using only right and up movements. We will show that the algorithm checks when such a path is possible, and, if it is, how to build a valid sequence of right ```R``` and up ```U``` movements to visit all the target locations in order.\n",
    "\n",
    "The algorithm works on a set of inputs whose general structure is known. It first reads the number $n$ of packages (1), and then their coordinates (4).\n",
    "\n",
    "Since the robot can move right and up, similar to a staircase, it is able to reach any package that is in the first quadrant of the moving reference system centered in the current robot's cell. To do this, the robot just needs to move the number of steps to the right that it takes to reach the package's x coordinate, and move up for a number of steps corresponding to the difference in y coordinates between robot and package.\n",
    "\n",
    "Every time there is at least one package in the robot's top-right quadrant, the algorithm must make sure that the robot does not skip packages, if it is avoidable. Otherwise, it might erroneously conclude that there is no path to reach all the packages. The sorting (6) ensures that the sequence of coordinates is non-decreasing in the x- and y-axes, and the robot visits them in the natural order tailored to its freedom of movement.\n",
    "\n",
    "Step (12) evaluates whether the next package in the sequence is reachable by the robot. If only one of the coordinates of the next package in line is smaller than the robot's current position coordinates, the algorithm assigns a ```False``` value to the variable ```possible``` and later prints 'NO' in (25).\n",
    "\n",
    "If all package coordinates are reachable in the sorted order, the algorithm constructs the path as follows:\n",
    "   - For each package coordinate $(x[i], y[i])$, add `R` repeated $ x[i] - \\text{x_current} $ times to move to the right (15)\n",
    "   - Then, add `U` repeated $ y[i] - \\text{y_current} $ times to move upward (16)\n",
    "\n",
    "This process continues for all packages until a complete path is generated from $(0, 0)$ to the final destination.\n",
    "\n",
    "The algorithm correctly identifies and implements a path when it exists because of the sorting step (6), the validity check in (12) and the path construction steps in (15)-(18). These passages enable the robot to traverse the grid and collect packages without backtracking.\n",
    "\n",
    "On the other hand, if a path through all the package coordinates does not exist, this fact will emerge from the check (12). It can happen when, for example, the next package in the sequence is close in x coordinate and high in the y coordinate, but a later package that is next in the x coordinate, has a lower y coordinate. This is because we prioritized horizontal coordinates over vertical ones. Thanks to (12), the algorithm will detect cases like these.\n",
    "\n",
    "**Termination**: The loop iterates through all $n$ packages exactly once, and each reachable package updates ```x_current``` and ```y_current``` without backtracking. Therefore, the algorithm terminates after a finite umber of steps.\n",
    "\n",
    "**Correct Path**: If the algorithm outputs 'YES' and a path, then all packages are reachable following the non-decreasing sequence of ```x``` and ```y``` coordinates. The resulting path ensures each package is visited in sorted order.\n",
    "\n",
    "**Verifying Existence**: If a path crossing all the packages starting from $(0,0)$ and moving only right and up does not exist, the algorithm detects it in the validity check and outputs 'NO'.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Algorithm Complexity\n",
    "\n",
    "Let us look at the algorithm step by step and calculate its complexity.\n",
    "\n",
    "* The algorithm starts with a loop over the test cases (0), so it is executed $t$ times. Therefore, the total running time of the algorithm will be the running time of the operations inside this for loop, times $t$\n",
    "* Afterwards, the algorithm reads $n$, the number of packages (1), and initializes the variable $\\text{packages}$ where the package coordinates will be stored (2). These two operations take constant time $O(1)$\n",
    "* Next, we find a for loop (3) that is executed $n$ times, once for each package whose coordinates need to be read. Reading the coordinates $x[i]$ and $y[i]$ happens in (4), then the coordinates are added to the $\\text{packages}$ array (5). Both (4) and (5) require constant time, and since the for loop is called $n$ times, for a single test case this segment will cost $O(n)$.\n",
    "* Step (6) sorts the package locations by x and y coordinates. This means that the coordinates are primarily sorted by their first entry, and if two locations have the same x coordinate, they will be sorted by their second coordinate. In the worst case, sorting will take $O(2 \\cdot n \\log(n))$, which is equivalent to $O(n \\log(n))$.\n",
    "* Steps (7) and (8) initialize the coordinates of the robot's current location, which initially is $(0,0)$, in $O(1)$ time\n",
    "* Step (9) initializes the $\\text{path}$ string ($O(1)$) where the moves of the robot will be saved, and step (10) initializes a boolean variable $\\text{possible}$ ($O(1)$), 'True' by default, which will be converted to 'False' in case it is not possible to construct the path through all the packages\n",
    "* After this, we have another for loop (11) that iterates over the coordinates in the $packages$ array and is executed at most $n$ times.\n",
    "* The for loop begins with an if loop (12) that checks if the coordinates $x[i]$ and $y[i]$ are out of reach for the robot from its current position. If they are reachable, the algorithm ignores the operations in the if cycle and continues to build the path. If the coordinates are not reachable, the $\\text{possible}$ variable is set to 'False' in (13) ($O(1)$) and the algorithm exits the for loop over the packages (14) ($O(1)$).\n",
    "* In case the coordinate validity check is positive, the algorithm continues to execute lines (15) to (20). Here, it first calculates how many steps the robot has to move right and up to reach the next package ((15) and (16), $O(1)$), then updates the $\\text{path}$ variable to include the new right and up moves ((17) and (18), $O(1)$). Finally, the current position of the robot is set to the current package's coordinates ((19) and (20), $O(1)$). All these operations together require twelve steps, considering addition, multiplication and assignement, so they all take constant time $O(1)$.\n",
    "* This concludes the for loop starting at (11), which executes constant operations at least $n$ times. Therefore, this loop costs $O(n)$ time in total.\n",
    "* At the end, we have an if loop (21) that checks if the $\\text{possible}$ variable is true. If yes, it prints 'YES' (22) and the previously computed $\\text{path}$ (23), requiring constant time $O(1)$. In case $\\text{possible}$ is False, an else block (24) is triggered, printing 'NO' (25). \n",
    "\n",
    "Putting all of this together, we get a worst-case total running time of $O(n\\log(n) + n)$ for each test case. Since $n\\log(n)$ is asymptotically stronger than $n$, we can rewrite this in big-O notation as $O(n\\log(n))$. This is the complexity for one test case. Considering all $t$ test cases, we get $O(t \\cdot n\\log(n))$. The most expensive part is the sorting step at (6), and all other segments are at most linear.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### LLM Complexity Evaluation\n",
    "\n",
    "Here is the complexity analysis of the algorithm provided by the LLM ChatGPT:\n",
    "\n",
    "### Step-by-Step Analysis of Time Complexity\n",
    "\n",
    "#### 1. Input Reading and Initialization (Steps 1-2):\n",
    "- Reading $n$ takes $O(1)$.\n",
    "- Initializing the `packages` list takes $O(1)$.\n",
    "\n",
    "#### 2. Reading Package Coordinates (Steps 3-5):\n",
    "- The loop runs $n$ times, and in each iteration:\n",
    "  - Reading $x[i]$ and $y[i]$ takes $O(1)$.\n",
    "  - Adding a tuple to the list takes $O(1)$.\n",
    "- **Total for this loop**: $O(n)$.\n",
    "\n",
    "#### 3. Sorting the Packages (Step 6):\n",
    "- Sorting $n$ elements based on two keys takes $O(n \\log n)$.\n",
    "\n",
    "#### 4. Path Construction and Validation (Steps 11-20):\n",
    "- The loop runs $n$ times, and in each iteration:\n",
    "  - Comparing $x[i]$ and $y[i]$ with `x_current` and `y_current` takes $O(1)$ (Step 12).\n",
    "  - Updating the `path` string involves concatenating strings for right and up moves. Let $k$ represent the total number of moves across all packages. Since $k \\leq n$ in the worst case:\n",
    "    - Adding characters to the string takes $O(k)$ in total.\n",
    "  - Updating `x_current` and `y_current` takes $O(1)$.\n",
    "- **Total for this loop**: $O(n)$.\n",
    "\n",
    "#### 5. Output Result (Steps 21-25):\n",
    "- Printing \"YES\" or \"NO\" and the path involves $O(n)$ for the path string output in the worst case.\n",
    "\n",
    "### Per Test Case Complexity:\n",
    "Summing the complexities for a single test case:\n",
    "- Reading input: $O(n)$\n",
    "- Sorting: $O(n \\log n)$\n",
    "- Path construction: $O(n)$\n",
    "- Outputs: $O(n)$\n",
    "\n",
    "Thus, the total complexity for a single test case is:\n",
    "$ O(n + n \\log n + n) = O(n \\log n)$\n",
    "\n",
    "### Accounting for $t$ Test Cases:\n",
    "With $t$ test cases, the above steps are repeated $t$ times. Therefore, the total complexity is: $O(t \\cdot n \\log n)$\n",
    "\n",
    "### Final Time Complexity:\n",
    "$O(t \\cdot n \\log n)$\n",
    "\n",
    "Both our analysis and the one provided by the LLM reach the same conclusion. The analysis is accurate because we are evaluating the worst-case running times in big-O notations for each test case, and then multiplying them by the number of test cases $t$. For each test case, we have: \n",
    "\n",
    "$O(1)$ (read $n$ and initialize $\\text{packages}$) $+ O(n)$ (read $2\\cdot n$ coordinates) $+O(n\\log(n))$ (sort coordinates) $+O(n)$ (for loop over coordinates with internal constant time operations) $+O(1)$ (final if-else block to print results) $=O(n\\log(n))$\n",
    "\n",
    "For $t$ test cases, this becomes $O(t \\cdot n\\log(n))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Greedy Algorithm\n",
    "\n",
    "A simplified pseudocode of a greedy algorithm that finds a path to collect all the packages is the following:\n",
    "\n",
    "### $ \\mathbf{GreedyPackageCollector(t)} $\n",
    "\n",
    "(0) $ \\textbf{for text case t} $:\n",
    "    \n",
    "* (1) $ read(n) $\n",
    "* (2) $ \\text{packages} \\leftarrow \\text{[ ]} $\n",
    "* (3) $ \\textbf{for i=1 to n}$: // $ O(n) $\n",
    "    * (4) $ read(x[i]), \\text{read}(y[i]) $\n",
    "    * (5) $ \\text{packages} \\leftarrow \\text{packages} + [(x[i], y[i])] $\n",
    "\n",
    "* (6) $ \\text{x_current} \\leftarrow 0 $\n",
    "* (7) $ \\text{y_current} \\leftarrow 0 $\n",
    "* (8) $ \\text{path} \\leftarrow \\text{''} $\n",
    "\n",
    "* (9) $ \\textbf{while packages} \\neq \\text{[ ]} $: // Continue until all packages are collected\n",
    "  * (10) $ \\text{min_distance} \\leftarrow \\infty $\n",
    "  * (11) $ \\text{closest_package} \\leftarrow \\text{None} $\n",
    "  * (12) $ \\textbf{for (x, y)  in packages} $: // $ O(k) $, where $ k $ is the number of remaining packages\n",
    "    * (13) $ \\text{distance} \\leftarrow |x - \\text{x_current}| + |y - \\text{y_current}| $\n",
    "    * (14) $ \\textbf{if distance} < \\textbf{min_distance} $:\n",
    "      * (15) $ \\text{min_distance} \\leftarrow \\text{distance} $\n",
    "      * (16) $ \\text{closest_package} \\leftarrow (x, y) $\n",
    "  * (17) $ (\\text{x_closest}, \\text{y_closest}) \\leftarrow \\text{closest_package} $\n",
    "  * (18) $ \\text{packages} \\leftarrow \\text{packages} ((\\text{x_closest}, \\text{y_closest})) $\n",
    "  \n",
    "  * (19) $ \\text{num_right_moves} \\leftarrow \\max(0, \\text{x_closest} - \\text{x_current}) $\n",
    "  * (20) $ \\text{num_left_moves} \\leftarrow \\max(0, \\text{x_current} - \\text{x_closest}) $\n",
    "  * (21) $ \\text{num_up_moves} \\leftarrow \\max(0, \\text{y_closest} - \\text{y_current}) $\n",
    "  * (22) $ \\text{num_down_moves} \\leftarrow \\max(0, \\text{y_current} - \\text{y_closest}) $\n",
    "\n",
    "  * (23) $ \\text{path} \\leftarrow \\text{path} + \\text{'R'} \\cdot \\text{num_right_moves} $\n",
    "  * (24) $ \\text{path} \\leftarrow \\text{path} + \\text{'L'} \\cdot \\text{num_left_moves} $\n",
    "  * (25) $ \\text{path} \\leftarrow \\text{path} + \\text{'U'} \\cdot \\text{num_up_moves} $\n",
    "  * (26) $ \\text{path} \\leftarrow \\text{path} + \\text{'D'} \\cdot \\text{num_down_moves} $\n",
    "\n",
    "  * (27) $ \\text{x_current} \\leftarrow \\text{x_closest} $\n",
    "  * (28) $ \\text{y_current} \\leftarrow \\text{y_closest} $\n",
    "\n",
    "* (29) $ output(\\text{path}) $\n",
    "\n",
    "$ \\textbf{end for} $\n",
    "\n",
    "\n",
    "This algorithm uses a greedy approach to move to the packages that are currently closest, minimizing the Manhattan distance at each step. It always calculates a path that includes all the packages because it can move in every direction, but the distance travelled is not always optimal.\n",
    "\n",
    "We can consider this counterexample: suppose the robot starts at position $(0,0)$ and the packages are distributed in these cells: $p_1:(1,0)$, $p_2:(2,0)$, $p_3:(3,0)$, $p_4:(4,0)$, $p_5: (5,0)$, $p_6: (6,0)$, $p_7: (7,0)$ and $p_8: (0,2)$. Initially, the algorithms calculates the distances between the robot and each package, and finds that the closest package is $p_1$, so it moves to $(1,0)$. After this, the algorithm searches for the next closest neighbour, which is $p_2$. Following the algorithm's logic, the robot will continue to collect packages $p_3$ to $p_7$ in order, and finally $p_8$.\n",
    "\n",
    "The total distance that the robot travels using the greedy approach in this scenario is 16 units (1 unit = length of a square in the grid), because it takes the path 'RRRRRRRLLLLLLLUU'. However, if the robot would have first visited package $p_2$, which initially was not the closest package, and then all the others in order, it would only have travelled a distance of 11 units along the path 'UUDDRRRRRRR'.\n",
    "\n",
    "The plot below illustrates this specific case of the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:28:52.800806Z",
     "start_time": "2024-11-17T21:28:52.041358Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Coordinates\n",
    "packages = [(1, 0), (2, 0), (3, 0), (4,0), (5,0), (6,0), (7, 0), (0, 2)]\n",
    "x_coords, y_coords = zip(*packages)\n",
    "robot = (0,0)\n",
    "\n",
    "# Plot points\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_coords, y_coords, color='blue', label='Points')\n",
    "plt.scatter(robot[0], robot[1], color='red', label='Robot', marker='x')\n",
    "\n",
    "# Add labels to points\n",
    "for x, y in packages:\n",
    "    plt.text(x + 0.1, y + 0.1, f\"({x},{y})\", fontsize=9)\n",
    "plt.text(0.1, 0.1, \"Robot\", fontsize=9)\n",
    "\n",
    "# Set plot limits for better visualization\n",
    "plt.xlim(-1, max(x_coords) + 2)\n",
    "plt.ylim(-1, max(y_coords) + 2)\n",
    "\n",
    "# Adjust plot\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Add grid and labels\n",
    "plt.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "plt.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, 9))\n",
    "plt.title('Robot in the Warehouse')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "The following code implements the PackageCollector algorithm explained in the first three points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:28:54.855480Z",
     "start_time": "2024-11-17T21:28:54.851519Z"
    }
   },
   "outputs": [],
   "source": [
    "def PackageCollector(t):\n",
    "    '''\n",
    "    Function that determines if a path traversing all the packages exists, and if it does, it outputs that path\n",
    "    Input:\n",
    "    t = number of test cases\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    for _ in range(t):  # Loop over each test case\n",
    "        n = int(input())  # Read the number of packages\n",
    "        packages = []\n",
    "\n",
    "        # Read packages\n",
    "        for i in range(n):\n",
    "            x, y = map(int, input().split())  # Read coordinates of the i-th package\n",
    "            packages.append((x, y))\n",
    "\n",
    "        packages = sorted(packages)  # Sort packages by x and y coordinates\n",
    "\n",
    "        # Initializations\n",
    "        x_current = 0  # current x-coordinate (starting at (0, 0))\n",
    "        y_current = 0  # current y-coordinate (starting at (0, 0))\n",
    "        path = \"\"  # initialize path\n",
    "        possible = True  # True by default, turns False if a path is not possible\n",
    "\n",
    "        for i in range(n):\n",
    "            x, y = packages[i]\n",
    "\n",
    "            if x < x_current or y < y_current: # check if package is reacheable\n",
    "                possible = False  # cannot reach this package if it's behind in x or y\n",
    "                break\n",
    "\n",
    "            # Calculate the number of right and up moves\n",
    "            num_right_moves = x - x_current\n",
    "            num_up_moves = y - y_current\n",
    "            \n",
    "            path += 'R' * num_right_moves # Add the right moves to the path\n",
    "            path += 'U' * num_up_moves # Add the up moves to the path\n",
    "\n",
    "            # Update the current position to the current package's coordinates\n",
    "            x_current = x\n",
    "            y_current = y\n",
    "\n",
    "        # Output results based on whether the path is possible\n",
    "        if possible:\n",
    "            print(\"YES\")\n",
    "            print(path)\n",
    "        else:\n",
    "            print(\"NO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we test the algorithm on a specific set of inputs:\n",
    "\n",
    "3\n",
    "\n",
    "5\n",
    "\n",
    "1 3\n",
    "\n",
    "1 2\n",
    "\n",
    "3 3\n",
    "\n",
    "5 5\n",
    "\n",
    "4 3\n",
    "\n",
    "2\n",
    "\n",
    "1 0\n",
    "\n",
    "0 1\n",
    "\n",
    "1\n",
    "\n",
    "4 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T21:29:19.087240Z",
     "start_time": "2024-11-17T21:28:57.583557Z"
    }
   },
   "outputs": [],
   "source": [
    "t = int(input())  # Number of test cases\n",
    "PackageCollector(t)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
